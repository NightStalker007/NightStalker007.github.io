<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>分类算法 | Night Stalker</title><meta name="description" content="分类算法sklearn 转换器和估计器转换器：我们把特征工程的接口称之为转换器，在特征工程中，我们常常实例化这样一个转换器类Transformer，然后再调用转换器方法fit_transform来对数据进行对应的计算处理与排版； 估计器：估计器(estimator)是一类实现分类算法的API;  用于分类的估计器：  sklearn.neighbors k-近邻算法 sklearn.naive_b"><meta name="keywords" content="机器学习"><meta name="author" content="NS"><meta name="copyright" content="NS"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://yoursite.com/2021/02/20/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="分类算法"><meta property="og:url" content="http://yoursite.com/2021/02/20/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"><meta property="og:site_name" content="Night Stalker"><meta property="og:description" content="分类算法sklearn 转换器和估计器转换器：我们把特征工程的接口称之为转换器，在特征工程中，我们常常实例化这样一个转换器类Transformer，然后再调用转换器方法fit_transform来对数据进行对应的计算处理与排版； 估计器：估计器(estimator)是一类实现分类算法的API;  用于分类的估计器：  sklearn.neighbors k-近邻算法 sklearn.naive_b"><meta property="og:image" content="https://pic4.zhimg.com/v2-7cfc909ebe8d83683909846edd6b5232_r.jpg?source=1940ef5c"><meta property="article:published_time" content="2021-02-20T12:19:41.263Z"><meta property="article:modified_time" content="2021-03-20T06:28:44.669Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="prev" title="TensorFlow学习笔记" href="http://yoursite.com/2021/02/27/TensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><link rel="next" title="特征工程" href="http://yoursite.com/2021/02/19/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: false,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true
  }</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.1"></head><body><canvas class="fireworks"></canvas><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://ss1.bdstatic.com/70cFuXSh_Q1YnxGkpoWK1HF6hhy/it/u=2700190935,3142448700&amp;fm=26&amp;gp=0.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">24</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">7</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">4</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#分类算法"><span class="toc-number">1.</span> <span class="toc-text">分类算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#sklearn-转换器和估计器"><span class="toc-number">1.1.</span> <span class="toc-text">sklearn 转换器和估计器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#K-近邻算法"><span class="toc-number">1.2.</span> <span class="toc-text">K-近邻算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#案例：鸢尾花种类预测"><span class="toc-number">1.2.1.</span> <span class="toc-text">案例：鸢尾花种类预测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#案例：水果成分分析"><span class="toc-number">1.2.2.</span> <span class="toc-text">案例：水果成分分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#优缺点"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">优缺点</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#模型评估"><span class="toc-number">1.3.</span> <span class="toc-text">模型评估</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#混淆矩阵"><span class="toc-number">1.3.1.</span> <span class="toc-text">混淆矩阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#精确率，召回率"><span class="toc-number">1.3.2.</span> <span class="toc-text">精确率，召回率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#API"><span class="toc-number">1.3.3.</span> <span class="toc-text">API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ROC曲线和AUC指标"><span class="toc-number">1.3.4.</span> <span class="toc-text">ROC曲线和AUC指标</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#TPR-FPR"><span class="toc-number">1.3.4.1.</span> <span class="toc-text">TPR,FPR</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ROC曲线与AUC"><span class="toc-number">1.3.4.2.</span> <span class="toc-text">ROC曲线与AUC</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#计算AUC"><span class="toc-number">1.3.4.3.</span> <span class="toc-text">计算AUC</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#模型选择与调优"><span class="toc-number">1.4.</span> <span class="toc-text">模型选择与调优</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#交叉验证"><span class="toc-number">1.4.1.</span> <span class="toc-text">交叉验证</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#网格搜索"><span class="toc-number">1.4.2.</span> <span class="toc-text">网格搜索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#API-1"><span class="toc-number">1.4.3.</span> <span class="toc-text">API:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#朴素贝叶斯"><span class="toc-number">1.5.</span> <span class="toc-text">朴素贝叶斯</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#理解"><span class="toc-number">1.5.1.</span> <span class="toc-text">理解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#拉普拉斯平滑系数"><span class="toc-number">1.5.2.</span> <span class="toc-text">拉普拉斯平滑系数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#API-2"><span class="toc-number">1.5.3.</span> <span class="toc-text">API</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#决策树算法"><span class="toc-number">1.6.</span> <span class="toc-text">决策树算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#信息熵与信息增益"><span class="toc-number">1.6.1.</span> <span class="toc-text">信息熵与信息增益</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#API-3"><span class="toc-number">1.6.2.</span> <span class="toc-text">API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#决策树可视化"><span class="toc-number">1.6.3.</span> <span class="toc-text">决策树可视化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#API-4"><span class="toc-number">1.6.3.1.</span> <span class="toc-text">API</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#优缺点-1"><span class="toc-number">1.6.4.</span> <span class="toc-text">优缺点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#集成学习方法之随机森林"><span class="toc-number">1.7.</span> <span class="toc-text">集成学习方法之随机森林</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#理解-1"><span class="toc-number">1.7.1.</span> <span class="toc-text">理解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#API-5"><span class="toc-number">1.7.2.</span> <span class="toc-text">API</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#模型保存和加载"><span class="toc-number">1.8.</span> <span class="toc-text">模型保存和加载</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#K-means算法"><span class="toc-number">1.9.</span> <span class="toc-text">K-means算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#过程"><span class="toc-number">1.9.1.</span> <span class="toc-text">过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#API-6"><span class="toc-number">1.9.2.</span> <span class="toc-text">API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-means评估指标"><span class="toc-number">1.9.3.</span> <span class="toc-text">K-means评估指标</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#轮廓系数"><span class="toc-number">1.9.3.1.</span> <span class="toc-text">轮廓系数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#轮廓系数API"><span class="toc-number">1.9.3.2.</span> <span class="toc-text">轮廓系数API</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-Means对Instacart-Market用户聚类"><span class="toc-number">1.9.4.</span> <span class="toc-text">K-Means对Instacart Market用户聚类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-Means对城市进行聚类"><span class="toc-number">1.9.5.</span> <span class="toc-text">K-Means对城市进行聚类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#K-means"><span class="toc-number">1.10.</span> <span class="toc-text">K-means++</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#系统聚类"><span class="toc-number">1.11.</span> <span class="toc-text">系统聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#肘部法则"><span class="toc-number">1.11.1.</span> <span class="toc-text">肘部法则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#对随机生成点进行系统聚类"><span class="toc-number">1.11.2.</span> <span class="toc-text">对随机生成点进行系统聚类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DBSCAN聚类算法"><span class="toc-number">1.12.</span> <span class="toc-text">DBSCAN聚类算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#优缺点-2"><span class="toc-number">1.12.1.</span> <span class="toc-text">优缺点</span></a></li></ol></li></ol></li></ol></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://pic4.zhimg.com/v2-7cfc909ebe8d83683909846edd6b5232_r.jpg?source=1940ef5c)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">Night Stalker</a></span><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">分类算法</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2021-02-20 20:19:41"><i class="far fa-calendar-alt fa-fw"></i> 发表于 2021-02-20</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2021-03-20 14:28:44"><i class="fas fa-history fa-fw"></i> 更新于 2021-03-20</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fas fa-inbox fa-fw post-meta__icon"></i><a class="post-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta__icon"></i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h1 id="分类算法"><a href="#分类算法" class="headerlink" title="分类算法"></a>分类算法</h1><h2 id="sklearn-转换器和估计器"><a href="#sklearn-转换器和估计器" class="headerlink" title="sklearn 转换器和估计器"></a>sklearn 转换器和估计器</h2><p><strong>转换器</strong>：我们把特征工程的接口称之为转换器，在特征工程中，我们常常实例化这样一个转换器类Transformer，然后再调用转换器方法fit_transform来对数据进行对应的计算处理与排版；</p>
<p><strong>估计器</strong>：估计器(estimator)是一类实现分类算法的API;</p>
<ul>
<li><p>用于分类的估计器：</p>
<ul>
<li>sklearn.neighbors k-近邻算法</li>
<li>sklearn.naive_bayes 贝叶斯</li>
<li>sklearn.linear_model.LogisticRegression 逻辑回归</li>
<li>sklearn.tree 决策树与随机森林</li>
</ul>
</li>
<li><p>用于回归的估计器：</p>
<ul>
<li>sklearn.linear_model.LinearRegression 线性回归</li>
<li>sklearn.linear_model.Ridge 岭回归</li>
</ul>
</li>
<li><p>用于无监督学习的估计器：</p>
<ul>
<li>sklearn.cluster.KMeans 聚类</li>
</ul>
</li>
</ul>
<p>估计器学习算法的实现：</p>
<p>1.实例化一个估计器estimator；<br>2.estimator.fit(x_train,y_train) 计算数据，生成模型；<br>3.模型评估：<br>    1）直接对比真实值和预测值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_predict = estimator.predict(x_test)</span><br><span class="line">y_test == y_predict</span><br></pre></td></tr></table></figure>

<p>​    2)计算准确率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accuracy = estimator.score(x_test,y_test)</span><br></pre></td></tr></table></figure>



<h2 id="K-近邻算法"><a href="#K-近邻算法" class="headerlink" title="K-近邻算法"></a>K-近邻算法</h2><p>KNN算法的定义：如果一个样本在特征空间中的k个最相似的（即特征空间中最邻近）的样本中的大多数属于某一个类别，那么这个样本也属于这个类别。</p>
<p>K值倘若取的过小容易受到异常点的影响，但K值过大会导致受样本不均衡的影响。</p>
<p>API:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sklearn.neighbors.KNeighborsClassifier(n_neighbors=<span class="number">5</span>,algorithm=<span class="string">'auto'</span>)</span><br><span class="line"><span class="comment"># n_neighbor是查询默认使用的近邻数量，algorithm是选择用于计算近邻的算法，有'ball_tree','kd_tree'等，‘auto’将会尝试根据传给fit方法的值来决定最合适的算法</span></span><br></pre></td></tr></table></figure>

<p> 处理过程：首先对数据集进行无量纲化，以避免数据范围差异造成的误差；然后再调用K近邻算法；</p>
<h3 id="案例：鸢尾花种类预测"><a href="#案例：鸢尾花种类预测" class="headerlink" title="案例：鸢尾花种类预测"></a>案例：鸢尾花种类预测</h3><p>Step:</p>
<ul>
<li><p>获取数据</p>
</li>
<li><p>数据集的划分</p>
</li>
<li><p>特征工程</p>
<ul>
<li>标准化</li>
</ul>
</li>
<li><p>KNN预估器流程</p>
</li>
<li><p>模型的评价</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">KNN_iris</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 获取数据</span></span><br><span class="line">    iris = load_iris()</span><br><span class="line">    <span class="comment"># 划分数据集</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=<span class="number">22</span>)</span><br><span class="line">    <span class="comment"># 'data':array([[],[],...,[]]),'target':array([])</span></span><br><span class="line">    <span class="comment"># 标准化</span></span><br><span class="line">    transfer = StandardScaler()</span><br><span class="line">    x_train = transfer.fit_transform(x_train)</span><br><span class="line">    x_test = transfer.transform(x_test) <span class="comment"># 测试集上的处理，只用标准化数据而不需要再次拟合数据</span></span><br><span class="line">    <span class="comment"># KNN</span></span><br><span class="line">    estimator = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)</span><br><span class="line">    estimator.fit(x_train,y_train)</span><br><span class="line">    <span class="comment"># 评估</span></span><br><span class="line">    <span class="comment"># 法1：比对真实值和预测值</span></span><br><span class="line">    y_predict = estimator.predict(x_test)</span><br><span class="line">    print(<span class="string">"y_predict:\n"</span>,y_predict,<span class="string">"y_test:\n"</span>,y_test)</span><br><span class="line">    print(<span class="string">"比对结果：\n"</span>,y_test == y_predict)</span><br><span class="line">    <span class="comment"># 法2:：计算准确率</span></span><br><span class="line">    rate = estimator.score(x_test,y_test)</span><br><span class="line">    print(<span class="string">"准确率为:\n"</span>,rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    KNN_iris()</span><br></pre></td></tr></table></figure>

<h3 id="案例：水果成分分析"><a href="#案例：水果成分分析" class="headerlink" title="案例：水果成分分析"></a>案例：水果成分分析</h3><p>数模训练时附加内容</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">()</span>:</span></span><br><span class="line">    e_xlsx = load_workbook(<span class="string">'D:\BaiduNetdiskDownload\正课视频的课件和代码\第9讲.分类模型\代码和例题数据\二分类数据\二分类水果数据.xlsx'</span>)</span><br><span class="line">    s1 = e_xlsx.active</span><br><span class="line">    a = np.zeros(shape=(<span class="number">38</span>,<span class="number">4</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">40</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">6</span>):</span><br><span class="line">            a[i<span class="number">-2</span>][j<span class="number">-2</span>] = s1.cell(i,j).value</span><br><span class="line">    b = np.arange(<span class="number">38</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">40</span>):</span><br><span class="line">        b[i<span class="number">-2</span>] = <span class="number">1</span> <span class="keyword">if</span> s1.cell(i,<span class="number">6</span>).value == <span class="string">'apple'</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(a, b, test_size=<span class="number">0.25</span>,random_state=<span class="number">22</span>)</span><br><span class="line">    transfer = StandardScaler()</span><br><span class="line">    x_train = transfer.fit_transform(x_train)</span><br><span class="line">    x_test = transfer.transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># KNN</span></span><br><span class="line">    estimator = KNeighborsClassifier()</span><br><span class="line">    estimator.fit(x_train,y_train)</span><br><span class="line">    y_predict = estimator.predict(x_test)</span><br><span class="line">    print(y_test == y_predict)</span><br><span class="line">    print(estimator.score(x_test,y_test))</span><br><span class="line">    print(classification_report(y_test, y_predict, labels=[<span class="number">0</span>,<span class="number">1</span>], target_names=[<span class="string">'l_0'</span>,<span class="string">'l_1'</span>]))</span><br><span class="line">    print(roc_auc_score(y_test, y_predict))</span><br><span class="line">    <span class="comment"># draw</span></span><br><span class="line">    fpr, tpr, thresholds = roc_curve(y_test, y_predict)</span><br><span class="line">    roc_auc = auc(fpr, tpr)</span><br><span class="line">    plt.title(<span class="string">'Receiver Operating Characteristic'</span>)</span><br><span class="line">    plt.plot(fpr, tpr, <span class="string">'#9400D3'</span>, label=<span class="string">u'AUC = %0.3f'</span> % roc_auc)</span><br><span class="line">    plt.legend(loc=<span class="string">'lower right'</span>)</span><br><span class="line">    plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">'r--'</span>)</span><br><span class="line">    plt.xlim([<span class="number">0.0</span>, <span class="number">1.1</span>])</span><br><span class="line">    plt.ylim([<span class="number">0.0</span>, <span class="number">1.1</span>])</span><br><span class="line">    plt.ylabel(<span class="string">'True Positive Rate'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'False Positive Rate'</span>)</span><br><span class="line">    plt.grid(linestyle=<span class="string">'-.'</span>)</span><br><span class="line">    plt.grid(<span class="literal">True</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="comment"># predict</span></span><br><span class="line">    eg = np.zeros(shape=(<span class="number">4</span>,<span class="number">4</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">40</span>,<span class="number">44</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">6</span>):</span><br><span class="line">            eg[i<span class="number">-40</span>][j<span class="number">-2</span>] = s1.cell(i,j).value</span><br><span class="line">    eg = transfer.transform(eg)</span><br><span class="line">    ans = estimator.predict(eg)</span><br><span class="line">    print(ans)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>



<h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><p>优点：</p>
<ul>
<li>简单易于理解，无需训练，易于实现</li>
</ul>
<p>缺点：</p>
<ul>
<li><p>懒惰算法，分类时计算量大，内存开销大</p>
</li>
<li><p>必须制定一个K值，所有要求对精度的把控到位。</p>
</li>
</ul>
<h2 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h2><h3 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h3><p>分类任务中，预测结果为正例时，若真实结果为正例，则为真正例TP，若真实结果为假例，则为伪正例FP；预测结果为假例时，若真实结果为正例，则为伪反例FN，若真实结果为假例，则为真反例TN；</p>
<h3 id="精确率，召回率"><a href="#精确率，召回率" class="headerlink" title="精确率，召回率"></a>精确率，召回率</h3><ul>
<li>精确率：预测结果为正例样本中真实结果为正例的比例；</li>
<li>召回率：真实结果为正例样本中预测结果为正例的比例；</li>
</ul>
<p>还有$F1-score$也可以作为模型评估标准，它反映模型的稳健性<br>$$<br>F1-score=\frac{2<em>Precision</em>Recall}{Precision+Recall}<br>$$</p>
<h3 id="API"><a href="#API" class="headerlink" title="API"></a>API</h3><ul>
<li><p>sklearn.metrics.classification_report(y_true, y_pred, labels=[], target_names=None)</p>
<ul>
<li>y_true：真实值</li>
<li>y_pred：预测值</li>
<li>labels：指定类别对应的数字</li>
<li>target_names：目标类别的名称</li>
<li>return：每个类别的精确率和召回率</li>
</ul>
<p>macro avg和weighted avg 是宏平均和加权平均；</p>
</li>
</ul>
<h3 id="ROC曲线和AUC指标"><a href="#ROC曲线和AUC指标" class="headerlink" title="ROC曲线和AUC指标"></a>ROC曲线和AUC指标</h3><h4 id="TPR-FPR"><a href="#TPR-FPR" class="headerlink" title="TPR,FPR"></a>TPR,FPR</h4><ul>
<li><p>TPR = TP / (TP + FN)</p>
<ul>
<li>所有真实类别为1的样本中，预测类别为1的比例</li>
</ul>
</li>
<li><p>FPR = FP / (FP + TN)</p>
<ul>
<li>所有真实类别为0的样本中，预测类别为1的比例</li>
</ul>
</li>
</ul>
<h4 id="ROC曲线与AUC"><a href="#ROC曲线与AUC" class="headerlink" title="ROC曲线与AUC"></a>ROC曲线与AUC</h4><p>曲线横轴为FPR，纵轴为TPR，二者相等时，有：无论真实类别为0或1的样本中预测类别为1的概率相等，此时AUC=0.5；</p>
<p>AUC越接近1分类器越优，越接近0.5分类器越糟糕；</p>
<h4 id="计算AUC"><a href="#计算AUC" class="headerlink" title="计算AUC"></a>计算AUC</h4><ul>
<li>from sklearn.metrics import roc_auc_score<ul>
<li>sklearn.metrics.roc_auc_score(y_true, y_score)<ul>
<li>计算ROC曲线的面积，即AUC值</li>
<li>y_true：样本真实类别，必须为0（反例）或1（正例）</li>
<li>y_score：预测得分，分类器方法的返回值</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="模型选择与调优"><a href="#模型选择与调优" class="headerlink" title="模型选择与调优"></a>模型选择与调优</h2><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><p>将训练数据分为训练集和测试集，例如将数据分为四份，依次以其中每一份为测试集，其他作为训练集，得到4组模型的训练结果，取平均值作为最终结果。</p>
<p>交叉验证的目的：让被评估的模型更加准确可信。</p>
<h3 id="网格搜索"><a href="#网格搜索" class="headerlink" title="网格搜索"></a>网格搜索</h3><p>对模型预设几种参数组合，每组超参数都采用交叉验证来进行评估，最后选出最优参数组合建立模型。</p>
<h3 id="API-1"><a href="#API-1" class="headerlink" title="API:"></a>API:</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sklearn.model_selection.GridSearchCV(estimator,param_grid=<span class="literal">None</span>,cv=<span class="literal">None</span>)</span><br><span class="line"><span class="comment"># estimator是估计器对象，param_grid是估计器的参数列表，cv指定几折交叉验证</span></span><br></pre></td></tr></table></figure>

<p>该方法还可以查看的量：</p>
<p>最佳参数：best_params_;最佳结果：best_score_;最佳估计器：best_estimator_;交叉验证结果：cv_results_</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">KNN_iris_gscv</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 获取数据</span></span><br><span class="line">    iris = load_iris()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 划分数据集</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=<span class="number">0.15</span>,random_state=<span class="number">22</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 标准化</span></span><br><span class="line">    transfer = StandardScaler()</span><br><span class="line">    x_train = transfer.fit_transform(x_train)</span><br><span class="line">    x_test = transfer.transform(x_test) <span class="comment"># 测试集上的处理，只用标准化数据而不需要再次拟合数据</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># KNN</span></span><br><span class="line">    estimator = KNeighborsClassifier()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加入网格搜索与交叉验证</span></span><br><span class="line">    param_dict = &#123;<span class="string">"n_neighbors"</span>: [<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>]&#125;</span><br><span class="line">    estimator = GridSearchCV(estimator,param_grid=param_dict,cv=<span class="number">10</span>)</span><br><span class="line">    estimator.fit(x_train,y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 评估</span></span><br><span class="line">    <span class="comment"># 法1：比对真实值和预测值</span></span><br><span class="line">    y_predict = estimator.predict(x_test)</span><br><span class="line">    print(<span class="string">"y_predict:\n"</span>,y_predict,<span class="string">"\ny_test:\n"</span>,y_test)</span><br><span class="line">    print(<span class="string">"\n比对结果：\n"</span>,y_test == y_predict)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 法2:：计算准确率</span></span><br><span class="line">    rate = estimator.score(x_test,y_test)</span><br><span class="line">    print(<span class="string">"\n准确率为:\n"</span>,rate,<span class="string">"\n最佳参数：\n"</span>,estimator.best_params_)</span><br><span class="line">    print(<span class="string">"\n最佳结果：\n"</span>,estimator.best_score_,<span class="string">"\n最佳估计器：\n"</span>,estimator.best_estimator_,<span class="string">"\n交叉验证结果：\n"</span>,estimator.cv_results_)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>



<h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><h3 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h3><p><a href="https://zhuanlan.zhihu.com/p/26262151" target="_blank" rel="noopener">理解朴素贝叶斯算法</a></p>
<p>贝叶斯公式：<br>$$<br>P(C|W)=\frac{P(W|C)P(C)}{P(W)}<br>$$<br>对于一个概率$P(A,B)$,若样本数据中没有$A,B$同时出现的样例，那么可以假设特征间相互独立，用$P(A,B)=P(A)*P(B)$计算。</p>
<p>用于文档分类则有如下公式：<br>$$<br>P(C|F1,F2,\cdots)=\frac{P(F1,F2,\cdots|C)P(C)}{P(F1,F2,\cdots)}<br>$$</p>
<ul>
<li>P(C)：每个文档是那个特定类别的概率；</li>
<li>P(F1,F2,$\cdots$|C)：给定了属于C类别下的各种特征的概率；</li>
<li>P(F1,F2,$\cdots$)：预测文档中每个词的概率</li>
</ul>
<p>但仅根据这个公式我们有时会由于样本过少而得到概率为0的结果，为了防止这个现象发生，我们引入拉普拉斯平滑系数；</p>
<h3 id="拉普拉斯平滑系数"><a href="#拉普拉斯平滑系数" class="headerlink" title="拉普拉斯平滑系数"></a>拉普拉斯平滑系数</h3><p>$$<br>P(F_i|C)=\frac{N_i+\alpha}{N+\alpha m}<br>$$</p>
<p>其中$\alpha$为指定系数，一般为1，$m$为训练文档中统计出的特征词种类个数（不包括重复）,$N_i,N$分别为$C$种类中特征词$F_i$个数和特征词总数（包括重复）。</p>
<h3 id="API-2"><a href="#API-2" class="headerlink" title="API"></a>API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sklearn.naive_bayes.MultinomialNB(alpha=<span class="number">1.0</span>)</span><br><span class="line"><span class="comment"># alpha是拉普拉斯平滑系数</span></span><br></pre></td></tr></table></figure>

<p>步骤分析：</p>
<ul>
<li><p>获取数据，划分训练集和测试集</p>
</li>
<li><p>特征工程</p>
<ul>
<li>文本特征抽取</li>
</ul>
</li>
<li><p>朴素贝叶斯预测</p>
</li>
<li><p>模型评估</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_20newsgroups</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nb_news</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 朴素贝叶斯对新闻进行分类</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取数据</span></span><br><span class="line">    news = fetch_20newsgroups(subset=<span class="string">'all'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 划分数据集</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(news.data, news.target)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 特征工程</span></span><br><span class="line">    transfer = TfidfVectorizer()</span><br><span class="line">    x_train = transfer.fit_transform(x_train)</span><br><span class="line">    x_test = transfer.transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># bayes</span></span><br><span class="line">    estimator = MultinomialNB()</span><br><span class="line">    estimator.fit(x_train,y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模型评估</span></span><br><span class="line">    <span class="comment"># Method1:直接比对</span></span><br><span class="line">    y_predict = estimator.predict(x_test)</span><br><span class="line">    print(<span class="string">"\ny_predict:\n"</span>,y_predict)</span><br><span class="line">    print(<span class="string">"\n比对结果：\n"</span>,y_predict == y_test)</span><br><span class="line">    <span class="comment"># Method2:计算准确率</span></span><br><span class="line">    rate = estimator.score(x_test,y_test)</span><br><span class="line">    print(<span class="string">"\nAccuracy:\n"</span>,rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    nb_news()</span><br></pre></td></tr></table></figure>



<h2 id="决策树算法"><a href="#决策树算法" class="headerlink" title="决策树算法"></a>决策树算法</h2><h3 id="信息熵与信息增益"><a href="#信息熵与信息增益" class="headerlink" title="信息熵与信息增益"></a>信息熵与信息增益</h3><p>信息熵的单位为比特：<br>$$<br>H(X)=-\sum_{i=1}^n P(x_i)log_bP(x_i)<br>$$<br>决策树的划分依据之一——<strong>信息增益</strong></p>
<p>定义：特征A对训练数据集D的信息增益g(D,A),定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)的差。<br>$$<br>g(D|A)=H(D)-H(D|A)<br>$$<br>条件熵的计算：<br>$$<br>H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)<br>$$<br>$D_i/D$为特征$i$的数据占比，</p>
<p>我们可以选择信息增益值最大的特征作为划分的第一个特征，逐步建立决策树。</p>
<h3 id="API-3"><a href="#API-3" class="headerlink" title="API"></a>API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.tree.DecisionTreeClassifier(criterion=<span class="string">'gini'</span>,max_depth=<span class="literal">None</span>,random_state=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>决策树分类器</li>
<li>critersion：默认是基尼系数，也可以选择信息增益的熵’entropy’</li>
<li>max_depth：树的深度大小</li>
<li>random_state：随机数种子</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decision_iris</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 决策树对鸢尾花数据集进行分类</span></span><br><span class="line">    <span class="comment"># 获取数据集</span></span><br><span class="line">    iris = load_iris()</span><br><span class="line">    <span class="comment"># 划分数据集</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=<span class="number">22</span>)</span><br><span class="line">    <span class="comment"># tree</span></span><br><span class="line">    estimator = DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>)</span><br><span class="line">    estimator.fit(x_train,y_train)</span><br><span class="line">    <span class="comment"># 模型评估</span></span><br><span class="line">    y_predict = estimator.predict(x_test)</span><br><span class="line">    print(<span class="string">"y_predict\n"</span>,y_predict)</span><br><span class="line">    print(<span class="string">"\n比较真实值与预测值:\n"</span>,y_predict == y_test)</span><br><span class="line"></span><br><span class="line">    rate = estimator.score(x_test,y_test)</span><br><span class="line">    print(<span class="string">"\n准确率为:\n"</span>,rate)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>



<h3 id="决策树可视化"><a href="#决策树可视化" class="headerlink" title="决策树可视化"></a>决策树可视化</h3><h4 id="API-4"><a href="#API-4" class="headerlink" title="API"></a>API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export_graphviz(estimator, out_file=<span class="string">'tree.dot'</span>,feature_names=[<span class="string">''</span>,<span class="string">''</span>])</span><br><span class="line">export_graphviz(dc, out_file=<span class="string">'./tree.dot'</span>, feature_names=[<span class="string">'age'</span>,<span class="string">'pclass=1'</span>])</span><br></pre></td></tr></table></figure>

<p>它会生成决策树的dot文件，在网站<a href="http://webgraphviz.com/" target="_blank" rel="noopener">WebGraphviz</a>中显示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export_graphviz(estimator, out_file=<span class="string">'iris_tree.dot'</span>,feature_names=iris.feature_names)</span><br></pre></td></tr></table></figure>



<h3 id="优缺点-1"><a href="#优缺点-1" class="headerlink" title="优缺点"></a>优缺点</h3><ul>
<li><p>优点</p>
<ul>
<li>简单，易于理解，可视化，因此可解释性强</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li>不能很好地推广数据过于复杂的树，这被称为过拟合</li>
</ul>
</li>
<li><p>改进</p>
<ul>
<li>剪枝cart算法（决策树的API中已经实现）</li>
<li>随机森林</li>
</ul>
</li>
</ul>
<h2 id="集成学习方法之随机森林"><a href="#集成学习方法之随机森林" class="headerlink" title="集成学习方法之随机森林"></a>集成学习方法之随机森林</h2><h3 id="理解-1"><a href="#理解-1" class="headerlink" title="理解"></a>理解</h3><p>随机森林是一个包含多个决策树的分类器，并且其输出的类别由个别树输出的类别的众数决定。</p>
<p>例如训练了5个树，4个认为是True，1个认为是False，那么就输出True.</p>
<ul>
<li><p>用N表示训练样本的个数，M表示特征数目</p>
<ul>
<li>一次选出一个样本，重复N次，也就是随机有放回地抽取N个样本作为新的数据集；</li>
<li>随机选出m个特征，m&lt;&lt;M,建立决策树</li>
</ul>
</li>
<li><p>采取bootstrap抽样</p>
</li>
</ul>
<h3 id="API-5"><a href="#API-5" class="headerlink" title="API"></a>API</h3><ul>
<li>class sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’,max_depth=None,bootstrap=True,random_state=None,min_samples_split=2)<ul>
<li>n_estimator：森林里的树木数量</li>
<li>criterion：分割特征的测量方法</li>
<li>max_depth：树的最大深度</li>
<li>max_features：每个决策树的最大特征数量</li>
<li>bootstrap：是否在建树时使用放回抽样</li>
<li>min_samples：节点划分最小样本数</li>
<li>min_samples_leaf：叶子结点的最小样本数</li>
</ul>
</li>
</ul>
<h2 id="模型保存和加载"><a href="#模型保存和加载" class="headerlink" title="模型保存和加载"></a>模型保存和加载</h2><ul>
<li>from sklearn.externals import joblib<ul>
<li>保存：joblib.dump(rf, ‘test.pkl’)</li>
<li>加载：estimator = joblib,load(‘test.pkl’)</li>
</ul>
</li>
</ul>
<h2 id="K-means算法"><a href="#K-means算法" class="headerlink" title="K-means算法"></a>K-means算法</h2><p>从无标签的数据开始学习并对其进行归纳和分组称为无监督学习，K-means属于无监督学习中的聚类算法。</p>
<h3 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h3><ul>
<li>随机设置K个特征空间内的点作为初始聚类中心点；</li>
<li>对于其他每个点计算到K个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别；</li>
<li>接着对标记的聚类中心之后，重新计算出每个聚类的新中心点（平均值）；</li>
<li>如果计算得到的中心点与原来的中心点一样，那么过程结束，否则进行第二步过程。</li>
</ul>
<h3 id="API-6"><a href="#API-6" class="headerlink" title="API"></a>API</h3><ul>
<li>sklearn.cluster.KMeans(n_clusters=8, init=’k-means++’)<ul>
<li>k-means 聚类</li>
<li>n_clusters: 开始的聚类中心数量</li>
<li>init: 初始化方法，默认为’k-means ++’</li>
<li>labels_: 默认标记的类型，可以和真实值比较</li>
</ul>
</li>
</ul>
<h3 id="K-means评估指标"><a href="#K-means评估指标" class="headerlink" title="K-means评估指标"></a>K-means评估指标</h3><h4 id="轮廓系数"><a href="#轮廓系数" class="headerlink" title="轮廓系数"></a>轮廓系数</h4><p>$$<br>sc_i=\frac{b_i-a_i}{max(b_i,a_i)}<br>$$</p>
<blockquote>
<p>对于每个点i为已完成聚类的数据中的样本，$b_i$为$i$到其他族群的所有样本的距离的最小值，$a_i$为$i$到本身簇的距离的平均值。最终计算出所有的样本点的轮廓系数平均值。</p>
</blockquote>
<p>遵从高内聚，低耦合的原理，我们希望得到模型的内部距离最小化，外部距离最大化的效果；即$b_i&gt;&gt;a_i$，即$sc_i\approx1$;聚类效果最差的时候则恰好相反，$sc_i\approx-1$；</p>
<h4 id="轮廓系数API"><a href="#轮廓系数API" class="headerlink" title="轮廓系数API"></a>轮廓系数API</h4><ul>
<li>sklearn.metrics.silhouette_score(X, labels)<ul>
<li>X: 特征值</li>
<li>labels: 被聚类标记的目标值</li>
</ul>
</li>
</ul>
<h3 id="K-Means对Instacart-Market用户聚类"><a href="#K-Means对Instacart-Market用户聚类" class="headerlink" title="K-Means对Instacart Market用户聚类"></a>K-Means对Instacart Market用户聚类</h3><p>我们不妨使用初始3个样本中心点，即K=3；</p>
<p>过程：</p>
<ul>
<li>预估计流程</li>
<li>获取结果</li>
<li>模型评估</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_score</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Kmeans</span><span class="params">()</span>:</span>-</span><br><span class="line">    estimator = KMeans(n_clusters=<span class="number">3</span>)</span><br><span class="line">    estimator.fit(data_new)</span><br><span class="line">    y_predict = estimator.predict(data_new)</span><br><span class="line">    rate = silhouette_score(data_new, y_predict)</span><br><span class="line">    print(<span class="string">"轮廓系数为："</span>,rate,<span class="string">"\n"</span>)</span><br></pre></td></tr></table></figure>

<h3 id="K-Means对城市进行聚类"><a href="#K-Means对城市进行聚类" class="headerlink" title="K-Means对城市进行聚类"></a>K-Means对城市进行聚类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">()</span>:</span></span><br><span class="line">    ex = load_workbook(<span class="string">'D:\BaiduNetdiskDownload\正课视频的课件和代码\第10讲.聚类模型\代码和例题数据\\1999年全国31个省份城镇居民家庭平均每人全年消费性支出数据 .xlsx'</span>)</span><br><span class="line">    s = ex.active</span><br><span class="line">    a = np.zeros(shape=(<span class="number">31</span>,<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">33</span>):</span><br><span class="line">        a[i<span class="number">-2</span>][<span class="number">0</span>] = s.cell(i,<span class="number">2</span>).value</span><br><span class="line">        a[i<span class="number">-2</span>][<span class="number">1</span>] = s.cell(i,<span class="number">7</span>).value</span><br><span class="line">    <span class="comment"># print(a)</span></span><br><span class="line">    estimator = KMeans(n_clusters=<span class="number">3</span>, init=<span class="string">'k-means++'</span>)</span><br><span class="line">    estimator.fit(a)</span><br><span class="line">    y_predict = estimator.predict(a)</span><br><span class="line">    rate = silhouette_score(a, y_predict)</span><br><span class="line">    print(y_predict)</span><br><span class="line">    print(rate)</span><br><span class="line">    city = [[] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>)]</span><br><span class="line">    color = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">33</span>):</span><br><span class="line">            city[y_predict[i<span class="number">-2</span>]].append(s.cell(i,<span class="number">1</span>).value)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        print(city[i])</span><br><span class="line">    <span class="comment"># draw</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">33</span>):</span><br><span class="line">        <span class="keyword">if</span> y_predict[i<span class="number">-2</span>] == <span class="number">0</span>:</span><br><span class="line">            color[i<span class="number">-2</span>] = <span class="string">'oy'</span></span><br><span class="line">        <span class="keyword">elif</span> y_predict[i<span class="number">-2</span>] == <span class="number">1</span>:</span><br><span class="line">            color[i<span class="number">-2</span>] = <span class="string">'ob'</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            color[i<span class="number">-2</span>] = <span class="string">'og'</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">33</span>):</span><br><span class="line">        plt.plot(s.cell(i,<span class="number">2</span>).value, s.cell(i,<span class="number">7</span>).value, color[i<span class="number">-2</span>], markersize=<span class="number">2.4</span>)</span><br><span class="line">    t = estimator.cluster_centers_</span><br><span class="line">    plt.plot(t[:,<span class="number">0</span>],t[:,<span class="number">1</span>],<span class="string">'r*'</span>,markersize=<span class="number">4.5</span>)</span><br><span class="line">    plt.title(<span class="string">'KMeans Clusters'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>



<h2 id="K-means"><a href="#K-means" class="headerlink" title="K-means++"></a>K-means++</h2><p>k-means++选择初始聚类中心的基本原则是：初始的聚类中心之间的相互距离要尽可能远；</p>
<ul>
<li>step1：随机选择一个样本点作为第一个聚类中心；</li>
<li>step2：计算每个样本点与当前已经选取的每一个聚类中心的距离的最短值，这个值越大表示被选取为下一个聚类中心的可能性越大；然后用轮盘法选取下一个中心；</li>
<li>重复step2直到选出K个聚类中心，选出初始中心后就可以进行K-means算法了。</li>
</ul>
<p><strong>注意</strong>：如果k-means使用的参数的量纲不一样，必须进行标准化，即<br>$$<br>z_i = \frac{x_i-\bar{x}}{\sigma_x}<br>$$</p>
<h2 id="系统聚类"><a href="#系统聚类" class="headerlink" title="系统聚类"></a>系统聚类</h2><p>系统聚类不需要提前指定将数据分为K类，而是在执行过程中进行不断的划分；整体流程如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">a[计算n个样本两两间的距离]--&gt;b[每个样本为一个类]</span><br><span class="line">b--&gt;c[每个类与最近的类合并]</span><br><span class="line">c--&gt;d[计算新类与当前所有类的距离]</span><br><span class="line">d--&gt;e&#123;类个数是否为1&#125;</span><br><span class="line">e--&gt;|no| c</span><br><span class="line">e--&gt;|yes| f[画聚类图]</span><br><span class="line">f--&gt;g[决定K值]</span><br></pre></td></tr></table></figure>

<h3 id="肘部法则"><a href="#肘部法则" class="headerlink" title="肘部法则"></a>肘部法则</h3><p>通过图形来大致估计最优的聚类数量；</p>
<p><strong>各类畸变程度之和</strong>：各个类的畸变程度等于该类的重心与其内部成员距离的平方和；</p>
<p>各个重心位置记为$u_k$,类总畸变程度$J$定义如下：<br>$$<br>J = \sum_{k=1}^K \sum_{i\in C_k} |x_i - u_k|^2<br>$$<br>聚合系数折线图：横坐标为聚合类数K，纵坐标为该K值下的总畸变程度；</p>
<p>得到折线图后，我们就根据肘部原则来选择K，我们观察聚合系数的下降趋势，在下降程度趋于缓慢的那几个结点中进行K值的选择。</p>
<h3 id="对随机生成点进行系统聚类"><a href="#对随机生成点进行系统聚类" class="headerlink" title="对随机生成点进行系统聚类"></a>对随机生成点进行系统聚类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.cluster.hierarchy <span class="keyword">import</span> dendrogram, linkage</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">()</span>:</span></span><br><span class="line">    state = np.random.RandomState(<span class="number">99</span>)  <span class="comment"># 设置随机状态</span></span><br><span class="line">    a = state.multivariate_normal([<span class="number">10</span>, <span class="number">10</span>], [[<span class="number">1</span>, <span class="number">3</span>],[<span class="number">3</span>, <span class="number">11</span>]], size=<span class="number">7</span>) <span class="comment"># 范围，协方差矩阵</span></span><br><span class="line">    b = state.multivariate_normal([<span class="number">-10</span>, <span class="number">-10</span>], [[<span class="number">1</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">11</span>]], size=<span class="number">8</span>)</span><br><span class="line">    data = np.concatenate((a, b))</span><br><span class="line">    <span class="comment"># draw</span></span><br><span class="line">    fig, ax = plt.subplots(figsize=(<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line">    ax.set_aspect(<span class="string">'equal'</span>)</span><br><span class="line">    plt.scatter(data[:,<span class="number">0</span>], data[:,<span class="number">1</span>])</span><br><span class="line">    plt.ylim([<span class="number">-30</span>, <span class="number">30</span>])  <span class="comment"># y轴范围</span></span><br><span class="line">    plt.xlim([<span class="number">-30</span>, <span class="number">30</span>])  <span class="comment"># x轴范围</span></span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="comment"># 聚类</span></span><br><span class="line">    z = linkage(data, <span class="string">'average'</span>)</span><br><span class="line">    <span class="comment"># draw</span></span><br><span class="line">    fig, ax = plt.subplots(figsize=(<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line">    dendrogram(z, leaf_font_size=<span class="number">14</span>)</span><br><span class="line">    plt.title(<span class="string">'Hierachial Clustering Dendrogram'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Cluster label'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Distance'</span>)</span><br><span class="line">    plt.axhline(y=<span class="number">10</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    f()</span><br></pre></td></tr></table></figure>



<h2 id="DBSCAN聚类算法"><a href="#DBSCAN聚类算法" class="headerlink" title="DBSCAN聚类算法"></a>DBSCAN聚类算法</h2><p><a href="https://blog.csdn.net/huacha__/article/details/81094891" target="_blank" rel="noopener">DBSCAN</a></p>
<p>DBSCAN聚类算法是一种基于密度的聚类算法；聚类前不需要指定聚类的个数，不指定生成的簇，它要求聚类空间中一定区域内所包含对象的数目不小于一个给定的阈值（即点密度），它能有效地应对异常数据；</p>
<p>DBSCAN中相关定义：</p>
<ul>
<li>核心点：在半径eps内点的数量不少于临界值</li>
<li>边界点：在半径eps内点的数量小于临界值，但它在核心点的邻域内；</li>
<li>噪音点：既不是核心点也不是边界点</li>
</ul>
<h3 id="优缺点-2"><a href="#优缺点-2" class="headerlink" title="优缺点"></a>优缺点</h3><ul>
<li>优点<ul>
<li>基于密度，能处理任意形状的簇</li>
<li>可以处理异常点</li>
<li>不需要事先界定划分的簇的个数</li>
</ul>
</li>
</ul>
<ul>
<li>缺点<ul>
<li>对输入参数，半径和临界值敏感，确定优秀的参数困难</li>
<li>当聚类的密度不均匀时聚类效果差</li>
<li>数据量大时，复杂度很大</li>
</ul>
</li>
</ul>
<p>综上，可以事先观察点的分布，即制作散点图，假若点的分布明显有同一类相距很近的情况（或者说非常具有形状特征），可以使用。否则还是使用系统聚类等。</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">NS</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://yoursite.com/2021/02/20/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/">http://yoursite.com/2021/02/20/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yoursite.com" target="_blank">Night Stalker</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="https://pic4.zhimg.com/v2-7cfc909ebe8d83683909846edd6b5232_r.jpg?source=1940ef5c" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/02/27/TensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><img class="prev-cover" data-src="https://pic4.zhimg.com/v2-7cfc909ebe8d83683909846edd6b5232_r.jpg?source=1940ef5c" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">TensorFlow学习笔记</div></div></a></div><div class="next-post pull-right"><a href="/2021/02/19/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"><img class="next-cover" data-src="https://pic4.zhimg.com/v2-7cfc909ebe8d83683909846edd6b5232_r.jpg?source=1940ef5c" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">特征工程</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2021/02/27/TensorFlow学习笔记/" title="TensorFlow学习笔记"><img class="relatedPosts_cover" data-src="https://pic4.zhimg.com/v2-7cfc909ebe8d83683909846edd6b5232_r.jpg?source=1940ef5c"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2021-02-27</div><div class="relatedPosts_title">TensorFlow学习笔记</div></div></a></div><div class="relatedPosts_item"><a href="/2021/02/13/logistic回归/" title="logistic回归"><img class="relatedPosts_cover" data-src="https://pic4.zhimg.com/v2-7cfc909ebe8d83683909846edd6b5232_r.jpg?source=1940ef5c"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2021-02-13</div><div class="relatedPosts_title">logistic回归</div></div></a></div><div class="relatedPosts_item"><a href="/2021/02/06/机器学习-线性回归/" title="机器学习-梯度下降"><img class="relatedPosts_cover" data-src="https://pic4.zhimg.com/v2-7cfc909ebe8d83683909846edd6b5232_r.jpg?source=1940ef5c"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2021-02-06</div><div class="relatedPosts_title">机器学习-梯度下降</div></div></a></div><div class="relatedPosts_item"><a href="/2021/02/19/特征工程/" title="特征工程"><img class="relatedPosts_cover" data-src="https://pic4.zhimg.com/v2-7cfc909ebe8d83683909846edd6b5232_r.jpg?source=1940ef5c"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2021-02-19</div><div class="relatedPosts_title">特征工程</div></div></a></div></div></div></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By NS</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><button id="readmode" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font_plus" title="放大字体"><i class="fas fa-plus"></i></button><button id="font_minus" title="缩小字体"><i class="fas fa-minus"></i></button><button class="translate_chn_to_cht" id="translateLink" title="简繁转换">繁</button><button id="darkmode" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="/js/third-party/fireworks.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><script>if (document.getElementsByClassName('mermaid').length) {
  loadScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js',function () {
    mermaid.initialize({
      theme: 'butterfly',
  })
})
}</script></body></html>